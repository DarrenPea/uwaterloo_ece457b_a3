# Assignment 3

## Q1 Default Network

Following the requirements in the provided architecture, this is the summary of the model:

![q1 model summary](img/q1/summary.png)

### Training and Validation Accuracy vs Epochs

![accuracy vs epochs](img/q1/accuracy.png)

### Training and Validation Loss vs Epochs

![loss vs epochs](img/q1/loss.png)

### Test Accuracy

After training the model, it was evaluated with the test set.

![test accuracy](img/q1/test.png)

## Q2 Your Own Improvements

1. We introduced a validation set along with the training and test set by splitting the training set into 80% training, 20% validation. This would help the model to tune its hyperparameters while training, before testing its generalisation performance on the test set.

2. Image augmentation was performed to increase the diversity of the training data and its size.
	```
	data_augmentation = ImageDataGenerator(
		rotation_range=15,
		width_shift_range=0.15,
		height_shift_range=0.15,
		shear_range=0.1,
		zoom_range=0.2,
		horizontal_flip=True,
	)
	```

3. We tried out a new architecture for the model by introducting `BatchNormalization` to stabilise and accelerate the training process through the normalising of inputs to each layer. Secondly, we introduced `Dropout` layers to prevent overfitting, by dropping a fraction of nodes during training. Thirdly, we increased the number of filters for each layer so that it will be able to extract more features from the dataset. Lastly, we tried using Adam optimiser for better learning. 
	```
	def build_improved_cnn():
		model = keras.Sequential([
			layers.Conv2D(64, (3, 3), input_shape=(28, 28, 1), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Conv2D(256, (3, 3), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Flatten(),
			layers.Dense(512, activation='relu'),
			layers.BatchNormalization(),
			layers.Dropout(0.25),
			layers.Dense(5, activation='softmax')
		])

		model.compile(
			optimizer=keras.optimizers.Adam(learning_rate=3e-4),
			loss='categorical_crossentropy',
			metrics=['accuracy']	
		)

		return model
	```

4. We also implemented callbacks like `ReduceLROnPlateau` to adjust the learning rate dynamically, and `EarlyStopping` to stop training when `val_loss` stops improving and restores the best weights.

## Q3 Result Analysis
### Model summary
This is the summary of the model:

![q2 model summary](img/q2/summary1.png)
![q2 model summary](img/q2/summary2.png)

### Runtime analysis

||Base Network|Improved Network|
|------------|------------|----------------|
|**Training Runtime**|            |                |
|**Testing Runtime**|            |                |

Comments on Runtimes:


### Hyper parameters and designs used
In our improved network, we tweaked the base network to include the following changes:
1. Increased Network depth and Complexity
	- The improved network used a deeper and wider network with 3 convolutional blocks, each increasing the number of filters (64 --> 128 --> 258). This design helps the network learn more complex features at different levels of abstraction.
2. Regularization
	- The improved network uses Dropout in several layers (0.15 in conv blocks and 0.25 before the final dense layer) and Batch Normalization after each Conv2D and Dense layer. These techniques help stabilize training and prevent overfitting by reducing internal covariate shift and randomly dropping out a subset of neurons during training.
3. Data Augmentation
	- The improved network implements a robust data augmentation strategy using random rotations, shifts, shearing, zoom, and horizontal flipping. This increases the effective size and variability of the training set, allowing the model to generalize better to unseen data.
4. Optimizer and Learning Rates
	- The improved network uses the Adam optimizer with an initial learning rate of 3e-4, along with a learning rate scheduler (`ReduceLROnPlateau`) that reduces the learning rate when the validation loss plateaus.
	- This allows the improved model to be superior to the base network which uses SGD, as SGD requires careful tuning of the learning rate and momentum, while Adam automatically adapts the learning rate during training—often resulting in faster convergence and better overall performance.
5. Training Epochs & Batch size
	- The improved network trains for up to 20 epochs (with EarlyStopping potentially stopping earlier) using a smaller batch size of 32, which is typical when using data augmentation as it provides more frequent weight updates and can capture more nuanced variations in the data.
	- The base network trains for 10 epochs with a batch size of 64, which is less ideal than the improved network because a larger batch size with fewer epochs results in fewer weight updates and can limit the model’s ability to learn from the increased diversity provided by data augmentatio
6. Validation Strategy
	- The improved network separates the training data into dedicated training and validation sets using `train_test_split` to monitor performance more robustly during data augmentation.
	- The base network uses a built-in validation split of 20% from the training data which may not be as effective in monitoring performance under augmented data conditions.



### Training and Validation Loss vs Epochs
![loss vs epochs](img/q2/loss.png)

### Training and Validation Classification Accuracy vs Epochs

![accuracy vs epochs](img/q2/accuracy.png)

### Test Accuracy

After training the model, it was evaluated with the test set.

![test accuracy](img/q2/test.png)

Through the improvements, the test accuracy increased from 0.9714 to 0.9847.