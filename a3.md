# Assignment 3

## Q1 Default Network

Following the requirements in the provided architecture, this is the summary of the model:

![q1 model summary](img/q1/summary.png)

### Training and Validation Accuracy vs Epochs

![accuracy vs epochs](img/q1/accuracy.png)

### Training and Validation Loss vs Epochs

![loss vs epochs](img/q1/loss.png)

### Test Accuracy

After training the model, it was evaluated with the test set.

![test accuracy](img/q1/test.png)

## Q2 Your Own Improvements

1. We introduced a validation set along with the training and test set by splitting the training set into 80% training, 20% validation. This would help the model to tune its hyperparameters while training, before testing its generalisation performance on the test set.

2. Image augmentation was performed to increase the diversity of the training data and its size.
	```
	data_augmentation = ImageDataGenerator(
		rotation_range=15,
		width_shift_range=0.15,
		height_shift_range=0.15,
		shear_range=0.1,
		zoom_range=0.2,
		horizontal_flip=True,
	)
	```

3. We tried out a new architecture for the model by introducting `BatchNormalization` to stabilise and accelerate the training process through the normalising of inputs to each layer. Secondly, we introduced `Dropout` layers to prevent overfitting, by dropping a fraction of nodes during training. Thirdly, we increased the number of filters for each layer so that it will be able to extract more features from the dataset. Lastly, we tried using Adam optimiser for better learning. 
	```
	def build_improved_cnn():
		model = keras.Sequential([
			layers.Conv2D(64, (3, 3), input_shape=(28, 28, 1), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Conv2D(256, (3, 3), strides=1, padding='same', activation='relu'),
			layers.BatchNormalization(),
			layers.MaxPooling2D((2, 2)),
			layers.Dropout(0.15),

			layers.Flatten(),
			layers.Dense(1024, activation='relu'),
			layers.BatchNormalization(),
			layers.Dropout(0.25),
			layers.Dense(5, activation='softmax')
		])

		model.compile(
			optimizer=keras.optimizers.Adam(learning_rate=3e-4),
			loss='categorical_crossentropy',
			metrics=['accuracy']	
		)

		return model
	```

4. We also implemented callbacks like `ReduceLROnPlateau` to adjust the learning rate dynamically, and `EarlyStopping` to stop training when `val_loss` stops improving and restores the best weights.

### Model Summary

This is the summary of the model:

![q2 model summary](img/q2/summary1.png)
![q2 model summary](img/q2/summary2.png)

### Training and Validation Accuracy vs Epochs

![accuracy vs epochs](img/q2/accuracy.png)

### Training and Validation Loss vs Epochs

![loss vs epochs](img/q2/loss.png)

### Test Accuracy

After training the model, it was evaluated with the test set.

![test accuracy](img/q2/test.png)

Through the improvements, the test accuracy increased from 0.9714 to 0.9847.